{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9dcc037450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, ToTensor\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "\n",
    "import binary as bin_eval\n",
    "\n",
    "manualSeed = 999\n",
    "\n",
    "print(\"Random Seed:\", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "test_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalFlip(object):\n",
    "    \"\"\"Horizontally flips the given PIL.Image with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "class VerticalFlip(object):\n",
    "    \"\"\"Vertically flips the given PIL.Image with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "       \n",
    "class ReLabel(object):\n",
    "\n",
    "    def __init__(self, nlabel):\n",
    "        self.nlabel = nlabel\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "    \n",
    "        tensor[tensor >= 0.1] = 1\n",
    "        tensor[tensor < 0.1] = 0\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "class Resize(object):\n",
    "    \"\"\"Vertically flips the given PIL.Image with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \n",
    "        img = np.array(img)\n",
    "        img = img[8:232, 8:232]\n",
    "        img = Image.fromarray(img)\n",
    "        return img\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        \n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "        \n",
    "        self.img_transform = Compose([\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.label_transform = Compose([\n",
    "            ToTensor(),\n",
    "            ReLabel(1),\n",
    "        ])\n",
    "    \n",
    "        self.input_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/train_data\"))))\n",
    "        self.label_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/train_lab\"))))\n",
    "        self.name = os.path.basename(root)\n",
    "        \n",
    "        if len(self.input_paths) == 0 or len(self.label_paths) == 0:\n",
    "            raise Exception(\"No images/labels are found in {}\".format(self.root))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = Image.open(self.input_paths[index])\n",
    "        label = Image.open(self.label_paths[index])\n",
    "               \n",
    "        if random.random() > 0.3:\n",
    "            image = np.array(image)\n",
    "            label = np.array(label)\n",
    "\n",
    "            s1 = random.randint(-35, 35)\n",
    "            s2 = random.randint(-35, 35)\n",
    "            s3 = random.randint(-35, 35)\n",
    "            s4 = random.randint(-35, 35)        \n",
    "\n",
    "            d1 = random.randint(210, 270)\n",
    "            d2 = random.randint(210, 270)\n",
    "            d3 = random.randint(210, 270)\n",
    "            d4 = random.randint(210, 270)\n",
    "\n",
    "            srcTri = np.array( [[0, 0], [239, 0], [0, 239], [239, 239]] ).astype(np.float32)\n",
    "            dstTri = np.array( [[s1, s2], [d1, s3], [s4, d3], [d2, d4]] ).astype(np.float32)\n",
    "\n",
    "            warp_mat = cv.getPerspectiveTransform(srcTri, dstTri)\n",
    "\n",
    "            image = cv.warpPerspective(image, warp_mat, (240, 240))\n",
    "            label = cv.warpPerspective(label, warp_mat, (240, 240))\n",
    "\n",
    "            image = Image.fromarray(image)\n",
    "            label = Image.fromarray(label)\n",
    "\n",
    "            #image.show()\n",
    "            #label.show()\n",
    "\n",
    "        image = Resize()(image)\n",
    "        label = Resize()(label)\n",
    "        \n",
    "        #randomly flip images\n",
    "        if random.random() > 0.5:\n",
    "            image = HorizontalFlip()(image)\n",
    "            label = HorizontalFlip()(label)\n",
    "              \n",
    "        if random.random() > 0.5:\n",
    "            image = VerticalFlip()(image)\n",
    "            label = VerticalFlip()(label)\n",
    "        \n",
    "        image = self.img_transform(image)\n",
    "        label = self.label_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    \n",
    "class Dataset_val(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        \n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "        \n",
    "        self.img_transform = Compose([\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.label_transform = Compose([\n",
    "            ToTensor(),\n",
    "            ReLabel(1),\n",
    "        ])\n",
    "        \n",
    "        #sort file names\n",
    "        self.input_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/val_data\"))))\n",
    "        self.label_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/val_lab\"))))\n",
    "        self.name = os.path.basename(root)\n",
    "        \n",
    "        if len(self.input_paths) == 0 or len(self.label_paths) == 0:\n",
    "            raise Exception(\"No images/labels are found in {}\".format(self.root))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = Image.open(self.input_paths[index])\n",
    "        label = Image.open(self.label_paths[index])\n",
    "        \n",
    "        image = Resize()(image)\n",
    "        label = Resize()(label)\n",
    "        \n",
    "        image = self.img_transform(image)\n",
    "        label = self.label_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "    \n",
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        \n",
    "        if not os.path.exists(self.root):\n",
    "            raise Exception(\"[!] {} not exists.\".format(root))\n",
    "        \n",
    "        self.img_transform = Compose([\n",
    "            ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.label_transform = Compose([\n",
    "            ToTensor(),\n",
    "            ReLabel(1),\n",
    "        ])\n",
    "        \n",
    "        #sort file names\n",
    "        self.input_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/test_data\"))), key=lambda x:int(x[32:-4]))\n",
    "        self.label_paths = sorted(glob(os.path.join(self.root, '{}/*.jpg'.format(\"data_jpg_all/test_lab\"))), key=lambda x:int(x[31:-4]))\n",
    "        self.name = os.path.basename(root)\n",
    "        \n",
    "        #print(self.input_paths)\n",
    "        #print(self.label_paths)\n",
    "        \n",
    "        if len(self.input_paths) == 0 or len(self.label_paths) == 0:\n",
    "            raise Exception(\"No images/labels are found in {}\".format(self.root))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = Image.open(self.input_paths[index])\n",
    "        label = Image.open(self.label_paths[index])\n",
    "        \n",
    "        image = Resize()(image)\n",
    "        label = Resize()(label)\n",
    "        \n",
    "        image = self.img_transform(image)\n",
    "        label = self.label_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loader(dataset, batch_size, num_workers=1, shuffle = False, drop_last=False):\n",
    "\n",
    "    input_images = dataset\n",
    "    input_loader = torch.utils.data.DataLoader(dataset=input_images, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, drop_last=drop_last)\n",
    "\n",
    "    return input_loader\n",
    "\n",
    "train_loader = loader(Dataset('../'), batch_size= batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = loader(Dataset_test('../'), batch_size= test_size, shuffle = False, drop_last=True)\n",
    "val_loader = loader(Dataset_val('../'), batch_size= test_size, shuffle = False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High-level Feature Enhancement Module\n",
    "\n",
    "class Vox_Att(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch):\n",
    "        super(Vox_Att, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, int(in_ch/2), kernel_size=1, padding=0),\n",
    "            nn.GroupNorm(8, int(in_ch/2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(int(in_ch/2), int(in_ch/2), kernel_size=3, padding=5, dilation=5),\n",
    "            nn.GroupNorm(8, int(in_ch/2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(int(in_ch/2), in_ch, kernel_size=1, padding=0),\n",
    "            nn.GroupNorm(8, in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x):\n",
    "        \n",
    "        x = self.conv1(in_x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return in_x * x + in_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class double_conv_HL(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv_HL, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        return x    \n",
    "    \n",
    "class single_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(single_conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        return x    \n",
    "    \n",
    "\n",
    "#Low-level Feature Enhancement Module  \n",
    "\n",
    "class High_map(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(High_map, self).__init__()\n",
    "        \n",
    "        self.Sconv1 = single_conv(in_ch, out_ch)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.Sconv2 = single_conv(out_ch, out_ch)       \n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.Sconv3 = single_conv(out_ch, out_ch*2)           \n",
    "        \n",
    "        self.Tconv1 = nn.ConvTranspose2d(out_ch*2, out_ch, kernel_size=2, stride=2, padding=0)\n",
    "        self.Sconv4 = single_conv(out_ch*2, out_ch)\n",
    "        \n",
    "        self.Tconv2 = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=2, stride=2, padding=0)\n",
    "        self.Sconv5 = single_conv(out_ch*2, out_ch)        \n",
    "        \n",
    "        self.conv6 = nn.Conv2d(out_ch, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.output = nn.Sigmoid()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        c1 = self.Sconv1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        \n",
    "        c2 = self.Sconv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "        \n",
    "        c3 = self.Sconv3(p2)\n",
    "        \n",
    "        t1 = self.Tconv1(c3)\n",
    "        x1 = torch.cat([c2, t1], dim=1)\n",
    "        c4 = self.Sconv4(x1)\n",
    "        \n",
    "        t2 = self.Tconv2(c4)\n",
    "        x2 = torch.cat([c1, t2], dim=1)\n",
    "        c5 = self.Sconv5(x2) \n",
    "        \n",
    "        c6 = self.conv6(c5) \n",
    "        \n",
    "        map_x = self.output(c6)\n",
    "        \n",
    "        return map_x\n",
    "\n",
    "      \n",
    "class High_to_Low(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(High_to_Low, self).__init__()\n",
    "        \n",
    "        self.Dconv1 = double_conv_HL(in_ch, out_ch)\n",
    "        \n",
    "        self.High_Map = High_map(in_ch, out_ch)\n",
    "        \n",
    "        self.Sconv1 = single_conv(out_ch, out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        D_x = self.Dconv1(x)\n",
    "        \n",
    "        H_x = self.High_Map(x)\n",
    "        \n",
    "        Mix_x = torch.add(D_x*H_x, D_x)\n",
    "        \n",
    "        out_x = self.Sconv1(Mix_x)\n",
    "        \n",
    "        return out_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 1\n",
    "numf =16\n",
    "\n",
    "class G_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_unet, self).__init__()\n",
    "        \n",
    "        self.Dconv1 = High_to_Low(input, numf)       \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.Dconv2 = High_to_Low(numf, numf*2)           \n",
    "        self.pool2 = nn.MaxPool2d(2)        \n",
    "        \n",
    "        self.Dconv3 = High_to_Low(numf*2, numf*4)          \n",
    "        self.pool3 = nn.MaxPool2d(2)     \n",
    "        \n",
    "        self.Dconv4 = High_to_Low(numf*4, numf*8)              \n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.Dconv5 = double_conv(numf*8, numf*16) \n",
    "        \n",
    "        self.Tconv1 = nn.ConvTranspose2d(numf*16, numf*8, kernel_size=2, stride=2, padding=0)\n",
    "        self.ACSgate1 = Vox_Att(numf*16)\n",
    "        self.Dconv6 = double_conv(numf*16, numf*8) \n",
    "        \n",
    "        self.Tconv2 = nn.ConvTranspose2d(numf*8, numf*4, kernel_size=2, stride=2, padding=0)\n",
    "        self.ACSgate2 = Vox_Att(numf*8)\n",
    "        self.Dconv7 = double_conv(numf*8, numf*4)        \n",
    "        \n",
    "        self.Tconv3 = nn.ConvTranspose2d(numf*4, numf*2, kernel_size=2, stride=2, padding=0)\n",
    "        self.ACSgate3 = Vox_Att(numf*4)\n",
    "        self.Dconv8 = double_conv(numf*4, numf*2)          \n",
    "        \n",
    "        self.Tconv4 = nn.ConvTranspose2d(numf*2, numf, kernel_size=2, stride=2, padding=0)\n",
    "        self.ACSgate4 = Vox_Att(numf*2)\n",
    "        self.Dconv9 = double_conv(numf*2, numf)   \n",
    "        \n",
    "        self.conv19 = nn.Conv2d(numf, 1, kernel_size=1, stride=1, padding=0)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        c1 = self.Dconv1(input)\n",
    "        p1 = self.pool1(c1)\n",
    "            \n",
    "        c2 = self.Dconv2(p1)\n",
    "        p2 = self.pool2(c2)  \n",
    "        \n",
    "        c3 = self.Dconv3(p2)\n",
    "        p3 = self.pool3(c3)  \n",
    "        \n",
    "        c4 = self.Dconv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "        \n",
    "        c5 = self.Dconv5(p4)\n",
    "        \n",
    "        t1 = self.Tconv1(c5)\n",
    "        x1 = torch.cat([c4, t1], dim=1)\n",
    "        x1 = self.ACSgate1(x1)\n",
    "        c6 = self.Dconv6(x1)\n",
    "        \n",
    "        t2 = self.Tconv2(c6)\n",
    "        x2 = torch.cat([c3, t2], dim=1)\n",
    "        x2 = self.ACSgate2(x2)\n",
    "        c7 = self.Dconv7(x2)         \n",
    "        \n",
    "        t3 = self.Tconv3(c7)\n",
    "        x3 = torch.cat([c2, t3], dim=1)\n",
    "        x3 = self.ACSgate3(x3)\n",
    "        c8 = self.Dconv8(x3)   \n",
    "        \n",
    "        t4 = self.Tconv4(c8)\n",
    "        x4 = torch.cat([c1, t4], dim=1)\n",
    "        x4 = self.ACSgate4(x4)\n",
    "        c9 = self.Dconv9(x4)\n",
    "        \n",
    "        c19 = self.conv19(c9)\n",
    "        \n",
    "        output = self.output(c19)\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_unet(\n",
      "  (Dconv1): High_to_Low(\n",
      "    (Dconv1): double_conv_HL(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "        (5): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (High_Map): High_map(\n",
      "      (Sconv1): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv2): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv3): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv1): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv4): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv2): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv5): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (conv6): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output): Sigmoid()\n",
      "    )\n",
      "    (Sconv1): single_conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Dconv2): High_to_Low(\n",
      "    (Dconv1): double_conv_HL(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        (5): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (High_Map): High_map(\n",
      "      (Sconv1): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv2): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv3): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv4): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv2): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv5): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (conv6): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output): Sigmoid()\n",
      "    )\n",
      "    (Sconv1): single_conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Dconv3): High_to_Low(\n",
      "    (Dconv1): double_conv_HL(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        (5): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (High_Map): High_map(\n",
      "      (Sconv1): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv2): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv3): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv4): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv2): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv5): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (conv6): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output): Sigmoid()\n",
      "    )\n",
      "    (Sconv1): single_conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Dconv4): High_to_Low(\n",
      "    (Dconv1): double_conv_HL(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        (5): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (High_Map): High_map(\n",
      "      (Sconv1): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv2): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Sconv3): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv4): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (Tconv2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (Sconv5): single_conv(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (conv6): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (output): Sigmoid()\n",
      "    )\n",
      "    (Sconv1): single_conv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Dconv5): double_conv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (Tconv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (ACSgate1): Vox_Att(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5))\n",
      "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      "  (Dconv6): double_conv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (Tconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (ACSgate2): Vox_Att(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5))\n",
      "      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      "  (Dconv7): double_conv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (Tconv3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (ACSgate3): Vox_Att(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5))\n",
      "      (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      "  (Dconv8): double_conv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (Tconv4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (ACSgate4): Vox_Att(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5))\n",
      "      (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      "  (Dconv9): double_conv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (conv19): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (output): Sigmoid()\n",
      ")\n",
      "# CFENet parameters: 4160037\n"
     ]
    }
   ],
   "source": [
    "CFENet = G_unet().to(device)\n",
    "\n",
    "print(CFENet)\n",
    "\n",
    "print('# CFENet parameters:', sum(param.numel() for param in CFENet.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "\n",
    "        return 1 - (2 * torch.sum(preds * labels) + self.smooth) / (torch.sum(preds) + torch.sum(labels) + self.smooth)\n",
    "\n",
    "\n",
    "class jaccard_Loss(nn.Module):\n",
    "    def __init__(self, smooth=1, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        \n",
    "        intersection = torch.sum(preds * labels)\n",
    "        return 1 - (intersection + self.smooth) / (torch.sum(preds) + torch.sum(labels) - intersection + self.smooth)    \n",
    "    \n",
    "\n",
    "def jaccard_coef(preds, labels):\n",
    "    \n",
    "    smooth=1\n",
    "    eps=1e-7\n",
    "    \n",
    "    intersection = torch.sum(preds * labels)\n",
    "    return (intersection + smooth) / (torch.sum(preds) + torch.sum(labels) - intersection + smooth)\n",
    "          \n",
    "def dice_coef(preds, labels):\n",
    "    \n",
    "    smooth=1\n",
    "    eps=1e-7\n",
    "    \n",
    "    return (2 * torch.sum(preds * labels) + smooth) / (torch.sum(preds) + torch.sum(labels) + smooth)\n",
    "\n",
    "\n",
    "def com_evaluation(preds, labels):\n",
    "    \n",
    "    smooth=1\n",
    "    eps=1e-7\n",
    "    \n",
    "    intersection = torch.sum(labels * preds)\n",
    "    num_pred = torch.sum(preds)\n",
    "    num_lab = torch.sum(labels)\n",
    "\n",
    "    num_TP = intersection\n",
    "    num_TN = (224*224*80) - (num_pred + num_lab - intersection)\n",
    "    \n",
    "    jaccard = (intersection + smooth) / (num_pred + num_lab - intersection + smooth)\n",
    "    dice = (2 * intersection + smooth) / (num_pred + num_lab + smooth)\n",
    "\n",
    "    pre_list = preds.numpy()\n",
    "    lab_list = labels.numpy()\n",
    "    assd = bin_eval.assd(pre_list, lab_list)\n",
    "    \n",
    "    return jaccard, dice, assd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Criterion = DiceLoss()\n",
    "\n",
    "C_optimizer = torch.optim.Adam(CFENet.parameters(), lr=0.0001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(C_optimizer, step_size=20, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    \n",
    "    run_dice_loss = 0.0\n",
    "    running_jaccard = 0.0\n",
    "\n",
    "    dice_list = []\n",
    "    jaccard_list = []\n",
    "    assd_list = []\n",
    "\n",
    "    valing_dice = 0.0\n",
    "    valing_jaccard = 0.0\n",
    "    valing_assd = 0.0\n",
    "\n",
    "    train_i = 0\n",
    "    test_i = 0\n",
    "    val_i = 0\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    for param_group in C_optimizer.param_groups:\n",
    "        if epoch % 20 == 0:\n",
    "            print(param_group['lr'])\n",
    "    \n",
    "    \n",
    "    for i_1, train_data in enumerate(train_loader):\n",
    "        \n",
    "        inputs, labels = train_data\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "               \n",
    "        pre_labs = CFENet(inputs)\n",
    "        dice_loss = Criterion(labels, pre_labs)\n",
    "        \n",
    "        jaccard = jaccard_coef(pre_labs, labels)\n",
    "\n",
    "        C_optimizer.zero_grad()\n",
    "        dice_loss.backward()\n",
    "        C_optimizer.step()\n",
    "\n",
    "        running_jaccard += jaccard\n",
    "        run_dice_loss += dice_loss.item()\n",
    "\n",
    "        train_i += 1\n",
    "        \n",
    "        if train_i == 600:\n",
    "            \n",
    "            lab_list = torch.zeros((80, 1, 224, 224), dtype = torch.float32)\n",
    "            pre_list = torch.zeros((80, 1, 224, 224), dtype = torch.float32)\n",
    "            \n",
    "            t_i = 0\n",
    "            for i_2, data in enumerate(test_loader):\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    test_data, test_lab = data\n",
    "\n",
    "                    test_data = test_data.to(device)\n",
    "                    test_lab = test_lab.to(device)\n",
    "\n",
    "                    pre_lab = CFENet(test_data)\n",
    "                    \n",
    "                    lab_list[t_i*10:t_i*10+10,:,:,:] = test_lab\n",
    "                    pre_list[t_i*10:t_i*10+10,:,:,:] = pre_lab\n",
    "                    \n",
    "                    t_i = t_i + 1\n",
    "                    \n",
    "                    if t_i%8 == 0:\n",
    "                        \n",
    "                        t_i = 0\n",
    "                        \n",
    "                        pre_list[pre_list < 0.5] = 0\n",
    "                        pre_list[pre_list >= 0.5] = 1\n",
    "                        jaccard, dice, assd = com_evaluation(pre_list, lab_list)\n",
    "                        \n",
    "                        dice_list.append(dice) \n",
    "                        jaccard_list.append(jaccard)\n",
    "                        assd_list.append(assd)\n",
    "                        \n",
    "                        Dice_npy = np.array(dice_list)\n",
    "                        Jaccard_npy = np.array(jaccard_list)\n",
    "                        Assd_npy = np.array(assd_list)\n",
    "                        \n",
    "                        test_i += 1\n",
    "\n",
    "                        \n",
    "            lab_list = torch.zeros((80, 1, 224, 224), dtype = torch.float32)\n",
    "            pre_list = torch.zeros((80, 1, 224, 224), dtype = torch.float32)                        \n",
    "            \n",
    "            t_i = 0\n",
    "            for i_3, v_data in enumerate(val_loader):\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    val_data, val_lab = v_data\n",
    "\n",
    "                    val_data = val_data.to(device)\n",
    "                    val_lab = val_lab.to(device)\n",
    "\n",
    "                    pre_lab = CFENet(val_data)\n",
    "                    \n",
    "                    lab_list[t_i*10:t_i*10+10,:,:,:] = val_lab\n",
    "                    pre_list[t_i*10:t_i*10+10,:,:,:] = pre_lab\n",
    "                    \n",
    "                    t_i = t_i + 1\n",
    "                    \n",
    "                    if t_i%8 == 0:\n",
    "                        \n",
    "                        t_i = 0\n",
    "                        \n",
    "                        pre_list[pre_list < 0.5] = 0\n",
    "                        pre_list[pre_list >= 0.5] = 1\n",
    "                        jaccard, dice, assd = com_evaluation(pre_list, lab_list)\n",
    "                        \n",
    "                        valing_dice += dice\n",
    "                        valing_jaccard += jaccard\n",
    "                        valing_assd += assd\n",
    "                        \n",
    "                        val_i += 1\n",
    "                        \n",
    "                        \n",
    "            list.append(valing_jaccard)\n",
    "\n",
    "            if  max(list) == valing_jaccard:\n",
    "\n",
    "                torch.save(CFENet, \"model/HL-LH.pkl\")\n",
    "                print(\"-------save %d epoch model---------\"% epoch)\n",
    "\n",
    "            print('e: %d, d_loss: %.5f, tr_jacc: %.5f, val_dice: %.5f, val_jacc: %.5f, val_assd: %.5f,' % \n",
    "                  (epoch, run_dice_loss/train_i, running_jaccard/train_i, valing_dice/val_i, valing_jaccard/val_i, valing_assd/val_i))\n",
    "            print('test_dice: %.5f, dice_std: %.5f, test_jacc: %.5f, jacc_std: %.5f, test_assd: %.5f, assd_std: %.5f,' % \n",
    "                  (Dice_npy.mean(), Dice_npy.std(), Jaccard_npy.mean(), Jaccard_npy.std(), Assd_npy.mean(), Assd_npy.std()))\n",
    "            print('-----------------------------------------------')\n",
    "            \n",
    "            run_dice_loss = 0.0\n",
    "            running_jaccard = 0.0\n",
    "            \n",
    "            dice_list = []\n",
    "            jaccard_list = []\n",
    "            assd_list = []\n",
    "            \n",
    "            valing_dice = 0.0\n",
    "            valing_jaccard = 0.0\n",
    "            valing_assd = 0.0\n",
    "\n",
    "            train_i = 0\n",
    "            test_i = 0\n",
    "            val_i = 0\n",
    "                          \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_11",
   "language": "python",
   "name": "pytorch_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
